# product.py
# terminal command: streamlit run product.py 

import os
import streamlit as st
from langchain_community.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

# í™˜ê²½ ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
openai_api_key = os.getenv("OPENAI_API_KEY")

# LangChain ChatOpenAI ì„¤ì •
chat_model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, openai_api_key=openai_api_key)

# Streamlit UI

st.title("ë”¥ëŸ¬ë‹ ê¸°ë°˜ ë‡Œì¢…ì–‘ ì§„ë‹¨ ë³´ê³ ì„œ")

# ì‚¬ìš©ì ì…ë ¥ê°’
st.header("MRI ë°ì´í„° ì…ë ¥")
distance = st.number_input("**ì¢…ì–‘ ì¤‘ì‹¬ê³¼ ë‡Œ ì¤‘ì‹¬ ê°„ ê±°ë¦¬ (í”½ì…€)**", min_value=0.0, format="%.2f")
size = st.number_input("**ì¢…ì–‘ í¬ê¸° (í”½ì…€ ìˆ˜)**", min_value=0, step=1)
irregularity = st.number_input("**ì¢…ì–‘ ë¶ˆê·œì¹™ì„± (ê¼­ì§“ì  ìˆ˜)**", min_value=0, step=1)
risk_score = st.number_input("**ìœ„í—˜ë„ ì ìˆ˜**", min_value=0.0, format="%.2f")

# ë²„íŠ¼ í´ë¦­ ì‹œ LLM í˜¸ì¶œ
if st.button("ê²°ê³¼ ë¶„ì„"):
    with st.spinner("LLM ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
        # LangChain ë©”ì‹œì§€ êµ¬ì„±
        system_message = SystemMessage(
            content=(
                "ë‹¹ì‹ ì€ ë‡Œì¢…ì–‘ ë¶„ì„ì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” ì˜ë£Œ ë³´ì¡°ì›ì…ë‹ˆë‹¤. "
                "MRI ë°ì´í„°ì—ì„œ ì¶”ì¶œí•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™˜ìì—ê²Œ ì¢…ì–‘ì˜ ìœ„ì¹˜, í¬ê¸°, ë¶ˆê·œì¹™ì„±, "
                "ê·¸ë¦¬ê³  ê³„ì‚°ëœ ìœ„í—˜ë„ ì ìˆ˜ì— ëŒ€í•œ ìƒì„¸í•œ í•´ì„ì„ ì œê³µí•©ë‹ˆë‹¤. "
                "ë˜í•œ, í™˜ìì˜ ìƒí™©ì— ì í•©í•œ ë‹¤ìŒ ë‹¨ê³„ì˜ ê¶Œê³  ì‚¬í•­ì„ ì œì•ˆí•´ì•¼ í•©ë‹ˆë‹¤."
            )
        )

        user_message = HumanMessage(
            content=(
                f"ë‹¤ìŒ MRI ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ ë‡Œì¢…ì–‘ì— ëŒ€í•œ ì˜í•™ì  í•´ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:\n\n"
                f"- ì¢…ì–‘ ì¤‘ì‹¬ê³¼ ë‡Œ ì¤‘ì‹¬ ê°„ ê±°ë¦¬: {distance:.2f} í”½ì…€\n"
                f"- ì¢…ì–‘ í¬ê¸°: {size} í”½ì…€\n"
                f"- ì¢…ì–‘ ë¶ˆê·œì¹™ì„±(ê¼­ì§“ì  ê°œìˆ˜): {irregularity}\n"
                f"- ìœ„í—˜ë„ ì ìˆ˜: {risk_score:.2f}\n\n"
                "st.subheader("ë¶„ì„ ê²°ê³¼") ì— ì œëª©ì´ ì í˜€ìˆìœ¼ë‹ˆ 'ì¢…ì–‘ì˜ ìœ„ì¹˜, í¬ê¸°, ë¶ˆê·œì¹™ì„± ë° ìœ„í—˜ë„ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼:' ì™€ ê°™ì€ ì œëª©ì€ ë°˜ë³µí•´ì„œ ì ìœ¼ë©´ ì•ˆ ë˜ê³ , ê° íŒŒíŠ¸ë³„ ë‚´ìš©ë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”.\n"
                "ì•„ë˜ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. ê° ì„¹ì…˜ì€ ë°˜ë“œì‹œ '###'ë¡œ êµ¬ë¶„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n"
                "### ì¢…ì–‘ì˜ ìœ„ì¹˜, í¬ê¸°, ë¶ˆê·œì¹™ì„± ë° ìœ„í—˜ë„ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n"
                "### ì¢…ì–‘ì˜ ìƒíƒœë¥¼ ê³ ë ¤í•˜ì—¬ í•„ìš”í•œ ì˜í•™ì  ê¶Œê³  ì‚¬í•­ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\n"
                "### í™˜ìì—ê²Œ ì „ë‹¬í•  ì¢…í•©ì ì¸ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\n"
            )
        )

        # LLM í˜¸ì¶œ ë° ì‘ë‹µ ì²˜ë¦¬
        response = chat_model([system_message, user_message])
        response_content = response.content

        # êµ¬ë¶„ì "###"ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒŒíŠ¸ ë‚˜ëˆ„ê¸°
        parts = [part.strip() for part in response_content.split("###") if part.strip()]

        # íŒŒíŠ¸ë³„ ë‚´ìš© ì¶œë ¥ (ì¤‘ë³µ ë°©ì§€)
        if len(parts) >= 3:
            # 1. ë¶„ì„ ê²°ê³¼
            st.subheader("ğŸ” ë¶„ì„ ê²°ê³¼")
            st.write(parts[0])  # ì²« ë²ˆì§¸ íŒŒíŠ¸

            st.divider()

            # 2. ì˜í•™ì  ê¶Œê³  ì‚¬í•­
            st.subheader("ğŸ©º ì˜í•™ì  ê¶Œê³  ì‚¬í•­")
            st.write(parts[1])  # ë‘ ë²ˆì§¸ íŒŒíŠ¸

            st.divider()

            # 3. ìµœì¢… ìš”ì•½
            st.subheader("ğŸ‘©â€âš•ï¸ ìµœì¢… ìš”ì•½")
            st.write(parts[2])  # ì„¸ ë²ˆì§¸ íŒŒíŠ¸
        else:
            st.error("LLM ì‘ë‹µì´ ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì…ë ¥ ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

